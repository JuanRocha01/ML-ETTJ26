{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d03a90",
   "metadata": {},
   "source": [
    "# **Trusted** \n",
    "\n",
    "A camada trusted como a primeira representação estruturada e validada dos dados após a ingestão bruta (raw). Ela transforma arquivos heterogêneos (JSON, CSV, XML, ZIP etc.) em datasets tabulares padronizados, tipados e auditáveis, mantendo fidelidade à fonte original.\n",
    "\n",
    "***Se o Raw é evidência, a Trusted é evidência organizada.***\n",
    "\n",
    "Ela ainda não aplica regras de negócio complexas ou integra múltiplas fontes — isso pertence às camadas seguintes (Refined, Feature, etc.). A Trusted apenas garante que os dados:\n",
    "\n",
    "- possuem tipos corretos\n",
    "- têm schema estável\n",
    "- são consistentes e reprocessáveis\n",
    "- mantêm rastreabilidade completa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451206e8",
   "metadata": {},
   "source": [
    "## *BCB SGS*\n",
    "\n",
    "### *1. Tipo e Estrutura*\n",
    "Os dados extraídos da SGS são até o momento especificamente as séries SELIC (432) e IPCA (433). Por definição esses são dados diários extraídos (para IPCA o BCB aplica um interpolação para retornar o variação percentual do dia) cujo o principal interesse é *data* e *valor*. Afim de preservar uma estrutura auditável também serão gerados campos que indiquem:\n",
    "\n",
    "- Data de Processamento (injestion)\n",
    "- Arquivo Raw que gerou aquele dados\n",
    "- Hash do conteúdo para governança, pois:\n",
    "    - Auditável permitindo reconstruir histórico, provar integridade e fazer reconciliação\n",
    "    - Caso reprocesse o mesmo período os dados não são reinseridos (indepotência)\n",
    "    - Detectar alteração silenciosa (Se houver alteração de dados antigos a coluna vai acusar)\n",
    "    - Unicidade Real; Pequenas variações de campo e formatação (como tipo) não impactam a base\n",
    "\n",
    "tornando assim os dados *raw* uma *tabela clean e auditável\". Com isso em mente podemos montar o schema desses dados como:\n",
    "\n",
    "**Raw:**\n",
    " data | valor |\n",
    "|:------------:|:-------:|\n",
    "| str | str |\n",
    "\n",
    "**Trusted**\n",
    "\n",
    "| series_id | ref_date | value | raw_file | raw_hash | record_hash | ijestioningestion_ts_utc |\n",
    "|:---------:|:--------:|:-----:|:--------:|:--------:|:-----------:|:------------------------:|\n",
    "| int | date | float | str | str | str | str |\n",
    "\n",
    "series_id e ref_date fornecem naturalmente uma chave única para a base de dados.\n",
    "\n",
    "#### *Classe Modelo*\n",
    "\n",
    "Como estamos falando de um processo fixo de injestão de dados que serão transformados em arquivos *.parquet* é interessenate construir uma classe fixa de formatação de dados não dependendo de pandas. Esse tipo de arquitetura é interessante inclusive pela restrição de flexbilidade do objeto gerando flexibilidade de código, já que seu futuramente pandas não for mais uma opção o domínio SgsPoint continua o mesmo, com controle de tipo e testabilidade.\n",
    "\n",
    "```python models.py\n",
    "@dataclass(frozen=True)\n",
    "class SgsPoint:\n",
    "    series_id: int\n",
    "    ref_date: date\n",
    "    value: Optional[float]  # pode ser None se vier vazio\n",
    "    raw_file: str\n",
    "    raw_hash: str\n",
    "    record_hash: str\n",
    "    ingestion_ts_utc: str \n",
    "```\n",
    "Da mesma forma, afim de manter a escalabilidade da Base de Dados podemos já esquematizar os Metadados da séries extraídas, gerando assim maior governança dos dados e facilidade de entendimente para futuros consumidores dessa informação, tendo assim noções de fonte, nome da série, frequência de publicação, unidade sem precisar consultar documentação.\n",
    "\n",
    "```python models.py\n",
    "@dataclass(frozen=True)\n",
    "class SgsSeriesMeta:\n",
    "    series_id: int\n",
    "    name: str\n",
    "    frequency: str\n",
    "    unit: str\n",
    "    source: str = \"BCB_SGS\" # Por padrão por enquanto\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabfbde8",
   "metadata": {},
   "source": [
    "Apartir da forma que os arquivos JSON estão sendo salvos podemos facilmente extrair de qual série são aqueles dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60db0b99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'433'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "path = Path(\"C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documentos\\\\GitHub\\\\ML-ETTJ26\\\\data\\\\01_raw\\\\bcb\\\\sgs\\\\433_01-01-2000_31-12-2008.json\")\n",
    "\n",
    "stem = path.stem\n",
    "series_str = stem.split(\"_\", 1)[0]\n",
    "series_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350c2db",
   "metadata": {},
   "source": [
    "\n",
    "Com uma rápida olhada nos dados podemos perceber que as informações extaráidas da API está em uma lista de discionários onde ambas as chaves e valores são strings.\n",
    "\n",
    "```JSON\n",
    "[{'data': '01/01/2000', 'valor': '1.00'}, ...]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba8dcdc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': '01/01/2000', 'valor': '0.62'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    json_f = json.load(f)\n",
    "json_f[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c20abe",
   "metadata": {},
   "source": [
    "Antes de passar esses valores para trusted eles precisam então ser normalizados:\n",
    "- Data deve ser formato date, não string\n",
    "- Valor deve ser valor decimal com quantidade fixa de casas, não string \n",
    "\n",
    "(obs: float pode gerar comportamento indesejado em razão da base binária)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9450e3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2000, 1, 1)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "s = json_f[0][\"data\"]\n",
    "datetime.strptime(s, \"%d/%m/%Y\").date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2fd3caba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6200000000 0.62\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal, ROUND_HALF_UP\n",
    "\n",
    "s = json_f[0].get(\"valor\")\n",
    "s = str(s).strip()\n",
    "s = Decimal(s)\n",
    "vq = s.quantize(Decimal(\"0.0000000001\"), rounding=ROUND_HALF_UP)\n",
    "print(vq, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa0e4a3",
   "metadata": {},
   "source": [
    "Tranquilamente conseguimos Gerar agora os valores hash com a segurança de comportamento maior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "599d1f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'072692bcfd6ededdac1377bf9cab985900fca426ef5d8543c6d9a7cdc779fb24'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "series_id = int(series_str)\n",
    "ref_date = datetime.strptime(json_f[0][\"data\"], \"%d/%m/%Y\").date()\n",
    "value_dec = vq\n",
    "\n",
    "payload = f\"{series_id}|{ref_date.isoformat()}|{value_dec}\"\n",
    "\n",
    "record_hash = hashlib.sha256(payload.encode(\"utf-8\")).hexdigest()\n",
    "record_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f602d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2064cccd061cb5ed9f8747e42c3cbbea8637b9542f78a6146547c9c8674f67ef'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = hashlib.sha256()\n",
    "with path.open(\"rb\") as f:\n",
    "    \n",
    "    for chunk in iter(lambda: f.read(1024 * 1024), b\"\"):\n",
    "        h.update(chunk)\n",
    "h.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8eef87fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2026-02-16T21:29:27+00:00'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timezone\n",
    "\n",
    "ingestion_ts_utc = datetime.now(timezone.utc).replace(microsecond=0).isoformat()\n",
    "ingestion_ts_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ec24f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 9852 entries, 0 to 9851\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   series_id         9852 non-null   int64  \n",
      " 1   ref_date          9852 non-null   object \n",
      " 2   value             9852 non-null   float64\n",
      " 3   record_hash       9852 non-null   str    \n",
      " 4   raw_file          9852 non-null   str    \n",
      " 5   raw_hash          9852 non-null   str    \n",
      " 6   ingestion_ts_utc  9852 non-null   str    \n",
      "dtypes: float64(1), int64(1), object(1), str(4)\n",
      "memory usage: 2.2+ MB\n",
      "None\n",
      "\n",
      "raw_hash\n",
      "480c424330ea60ca1d9e880bb515e8be38bb0b61f0c3d6f6b07d640e8884bad9    3329\n",
      "023c3e8e15126a84ae1b8a9e00a325719c03eb7c8f8bbb7bf1988a81415c7926    3288\n",
      "74ec4cb5fdcd6e61a207f15f04df8b708d760c50f50e8cb4a1bd6538b7db72c6    2922\n",
      "2064cccd061cb5ed9f8747e42c3cbbea8637b9542f78a6146547c9c8674f67ef     108\n",
      "93a1f52671c819997afbd5e454898679dd21e844a227298538b613a37dae725e     108\n",
      "7a8c38a1e810ad0d8478c2962c395b58dfff765e62ade8eaddc73a51771c4f0c      97\n",
      "Name: count, dtype: int64\n",
      "\n",
      "raw_file\n",
      "432_01-01-2017_11-02-2026.json    3329\n",
      "432_01-01-2000_31-12-2008.json    3288\n",
      "432_01-01-2009_31-12-2017.json    2922\n",
      "433_01-01-2000_31-12-2008.json     108\n",
      "433_01-01-2009_31-12-2017.json     108\n",
      "433_01-01-2018_13-02-2026.json      97\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Após rodar o pipeline da trusted ( kedro run --pipeline trusted_bcb_sgs ) podemos ler o parquet que deve ser gerado\n",
    "import pandas as pd\n",
    "\n",
    "sgs_p = pd.read_parquet(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\02_trusted\\bcb\\sgs\\points.parquet\")\n",
    "print(sgs_p.info(), sgs_p[\"raw_hash\"].value_counts(), sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0af698c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 2 entries, 0 to 1\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype\n",
      "---  ------       --------------  -----\n",
      " 0   series_id    2 non-null      int64\n",
      " 1   series_name  2 non-null      str  \n",
      " 2   frequency    2 non-null      str  \n",
      " 3   unit         2 non-null      str  \n",
      " 4   source       2 non-null      str  \n",
      "dtypes: int64(1), str(4)\n",
      "memory usage: 244.0 bytes\n",
      "None\n",
      "\n",
      "   series_id series_name frequency    unit   source\n",
      "0        432       SELIC         D  % a.a.  BCB_SGS\n",
      "1        433        IPCA         M       %  BCB_SGS\n"
     ]
    }
   ],
   "source": [
    "sgs_m = pd.read_parquet(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\02_trusted\\bcb\\sgs\\series_meta.parquet\")\n",
    "print(sgs_m.info(), sgs_m.head(), sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc32a941",
   "metadata": {},
   "source": [
    "## *BCB DEMAB*\n",
    "\n",
    "Ao contrario do Sistema Gerenciador de Séries Temporais (SGS) do BCB o Departamento de Operações do Mercado Aberto (DEMAB) oferece os dados mensais de definitivos de Títulos Públicos Federais registrados no Sistema Especial de Liquidação e Custódia (o Selic), com detalhamento diário por título e vencimento,  ou seja, o exato insumo nescessário para construção da ETTJ PRE desse projeto.\n",
    "\n",
    "Obs: Considerarei apenas os negociados Extragrupo por considerar que as informações Intragrupo (NegT) pode gerar uma espécie de ruído de preço e liquidez desnecessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "effa4e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP: NegE200701.ZIP\n",
      "Counter({'.csv': 1})\n",
      "Arquivos: 1\n",
      "- NegE200701.CSV | 94.2 KB | 2007-02-03 00:04:04\n",
      "ZIP: NegE202309.ZIP\n",
      "Counter({'.csv': 1})\n",
      "Arquivos: 1\n",
      "- NegE202309.CSV | 142.7 KB | 2023-10-03 23:47:42\n",
      "ZIP: NegE202512.ZIP\n",
      "Counter({'.csv': 1})\n",
      "Arquivos: 1\n",
      "- NegE202512.CSV | 166.5 KB | 2026-01-05 23:04:18\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "zip_path = [Path(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\01_raw\\bcb\\demab\\negociacoes_titulos_federais_secundario\\NegE200701.ZIP\"), \n",
    "            Path(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\01_raw\\bcb\\demab\\negociacoes_titulos_federais_secundario\\NegE202309.ZIP\"),  \n",
    "            Path(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\01_raw\\bcb\\demab\\negociacoes_titulos_federais_secundario\\NegE202512.ZIP\"),\n",
    "            ]\n",
    "\n",
    "for zp in zip_path:\n",
    "    with zipfile.ZipFile(zp, \"r\") as z:\n",
    "        infos = z.infolist()\n",
    "        print(f\"ZIP: {zp.name}\")\n",
    "        exts = [Path(i.filename).suffix.lower() for i in z.infolist() if not i.is_dir()]\n",
    "        print(Counter(exts))\n",
    "        print(f\"Arquivos: {len(infos)}\")\n",
    "        for i in infos[:50]:\n",
    "            dt = datetime(*i.date_time)\n",
    "            print(f\"- {i.filename} | {i.file_size/1024:.1f} KB | {dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316e3f8b",
   "metadata": {},
   "source": [
    "Rapidamente podemos conferir os arquivos em diferentes momentos do tempo explorando e garantido se há consistência na forma que os arquivos foram extraídos e como podemos ver, além do nome *NegEyyymm.ZIP* o arquivo compactado parece se manter o mesmo ao longo do tempo ( .csv ) sendo sempre único"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d917ed7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo: NegE200701.CSV\n",
      "Encoding provável: utf-8\n",
      "['DATA MOV;SIGLA;CODIGO;CODIGO ISIN;EMISSAO;VENCIMENTO;NUM DE OPER;QUANT NEGOCIADA;VALOR NEGOCIADO;PU MIN;PU MED;PU MAX;PU LASTRO;VALOR PAR;TAXA MIN;TAXA MED;TAXA MAX', '02/01/2007;LFT;210100;BRSTNCLF1741;04/01/2002;17/01/2007;28;4223;;2963,02918000;2963,02918000;2963,02918000;2962,89340793;2963,00978687;-0,0150;-0,0150;-0,0150', '02/01/2007;LFT;210100;BRSTNCLF17W8;19/09/2002;21/02/2007;2;1259;;2963,06973000;2963,07607744;2963,07694900;2962,65014685;2963,00978687;-0,0168;-0,0166;-0,0150', '02/01/2007;LFT;210100;BRSTNCLF17X6;19/09/2002;21/03/2007;43;14756;;2963,00978600;2963,09021875;2963,09125200;2962,37521180;2963,00978687;-0,0128;-0,0127;0,0000', '02/01/2007;LFT;210100;BRSTNCLF1808;19/09/2002;20/06/2007;11;12764;;2963,00978600;2963,15668471;2963,17210800;2961,64682719;2963,00978687;-0,0119;-0,0108;0,0000', '02/01/2007;LFT;210100;BRSTNCLF1832;19/09/2002;19/09/2007;4;1113;;2963,23409641;2963,23620561;2963,23646600;2960,89513416;2963,00978687;-0,0107;-0,0107;-0,0106', '02/01/2007;LFT;210100;BRSTNCLF1865;20/09/2002;19/12/2007;4;10208;;2963,00978687;2963,29982052;2963,30004800;2960,16711347;2963,00978687;-0,0102;-0,0102;0,0000', '02/01/2007;LFT;210100;BRSTNCLF1899;20/09/2002;19/03/2008;7;8461;;2963,28093300;2963,28093300;2963,28093300;2959,09556558;2963,00978687;-0,0076;-0,0076;-0,0076', '02/01/2007;LFT;210100;BRSTNCLF18C8;20/09/2002;18/06/2008;6;11053;;2967,29064080;2967,29064080;2967,29064080;2958,30818549;2963,00978687;-0,0999;-0,0999;-0,0999', '02/01/2007;LFT;210100;BRSTNCLF18F1;20/09/2002;17/09/2008;6;122;;2963,00978687;2963,58861971;2964,55334112;2957,46940448;2963,00978687;-0,0306;-0,0115;0,0000']\n",
      "Arquivo: NegE202309.CSV\n",
      "Encoding provável: utf-8\n",
      "['DATA MOV;SIGLA;CODIGO;CODIGO ISIN;EMISSAO;VENCIMENTO;NUM DE OPER;QUANT NEGOCIADA;VALOR NEGOCIADO;PU MIN;PU MED;PU MAX;PU LASTRO;VALOR PAR;TAXA MIN;TAXA MED;TAXA MAX;NUM OPER COM CORRETAGEM;QUANT NEG COM CORRETAGEM', '01/09/2023;LFT;210100;BRSTNCLF1RA8;05/01/2018;01/03/2024;37;69323;;13748,27389400;13749,10353500;13751,87601300;13743,22718972;13748,54886500;-0,0497;-0,0082;0,0043;14;16093', '01/09/2023;LFT;210100;BRSTNCLF0008;06/07/2018;01/09/2024;95;358838;;13747,86143700;13749,20258800;13822,18609200;13737,61877107;13748,54886500;-0,5368;-0,0047;0,0051;21;26026', '01/09/2023;LFT;210100;BRSTNCLF1RC4;26/10/2018;01/03/2025;72;68970;;13736,72511200;13748,23983700;13758,83277900;13730,53021963;13748,54886500;-0,0499;0,0015;0,0575;18;21016', '01/09/2023;LFT;210100;BRSTNCLF1RD2;08/03/2019;01/09/2025;131;316528;;13745,52425100;13747,35694600;13753,05838900;13723,70922581;13748,54886500;-0,0164;0,0043;0,0110;19;14820', '01/09/2023;LFT;210100;BRSTNCLF1RE0;06/09/2019;01/03/2026;135;349932;;13732,77927900;13734,35578900;13736,02393600;13706,01852313;13748,54886500;0,0366;0,0415;0,0461;22;29107', '01/09/2023;LFT;210100;BRSTNCLF1RF7;13/03/2020;01/09/2026;52;73052;;13718,54953100;13721,53956800;13744,42430000;13685,80169553;13748,54886500;0,0100;0,0657;0,0730;2;900', '01/09/2023;LFT;210100;BRSTNCLF1RG5;03/07/2020;01/03/2027;92;424191;;13688,78392400;13694,80816500;13710,38289300;13655,66387881;13748,54886500;0,0800;0,1127;0,1254;15;250810', '01/09/2023;LFT;210100;BRSTNCLF1RH3;02/07/2021;01/09/2027;57;93099;;13674,77415100;13679,48168300;13695,47946600;13634,38514234;13748,54886500;0,0970;0,1263;0,1350;5;11166', '01/09/2023;LFT;210100;BRSTNCLF1RI1;05/01/2022;01/03/2028;25;42489;;13658,53711500;13661,06151300;13662,88165700;13610,26131961;13748,54886500;0,1396;0,1426;0,1467;3;9864']\n",
      "Arquivo: NegE202512.CSV\n",
      "Encoding provável: utf-8\n",
      "['DATA MOV;SIGLA;CODIGO;CODIGO ISIN;EMISSAO;VENCIMENTO;NUM DE OPER;QUANT NEGOCIADA;VALOR NEGOCIADO;PU MIN;PU MED;PU MAX;PU LASTRO;VALOR PAR;TAXA MIN;TAXA MED;TAXA MAX;NUM OPER COM CORRETAGEM;QUANT NEG COM CORRETAGEM', '01/12/2025;LFT;210100;BRSTNCLF1RE0;06/09/2019;01/03/2026;49;293211;;17872,87366400;17876,94007800;17877,62879300;17873,48981452;17877,62879300;0,0000;0,0161;0,1099;6;8743', '01/12/2025;LFT;210100;BRSTNCLF1RF7;13/03/2020;01/09/2026;17;2232;;17877,62879300;17878,35973100;17882,40211900;17868,32504148;17877,62879300;-0,0356;-0,0053;0,0000;7;1104', '01/12/2025;LFT;210100;BRSTNCLF1RG5;03/07/2020;01/03/2027;55;114533;;17868,85087700;17871,62980600;17871,78280800;17854,15708383;17877,62879300;0,0267;0,0274;0,0401;12;3745', '01/12/2025;LFT;210100;BRSTNCLF1RH3;02/07/2021;01/09/2027;47;196126;;17849,69749300;17865,24137000;17865,79380200;17840,43330495;17877,62879300;0,0381;0,0398;0,0900;7;2158', '01/12/2025;LFT;210100;BRSTNCLF1RI1;05/01/2022;01/03/2028;36;92307;;17855,94322900;17857,28614400;17857,85613500;17825,46975799;17877,62879300;0,0496;0,0510;0,0544;5;1471', '01/12/2025;LFT;210100;BRSTNCLF1RK7;06/04/2022;01/09/2028;46;19089;;17828,26865900;17850,07897100;17851,11626900;17811,12736272;17877,62879300;0,0542;0,0563;0,1010;6;3138', '01/12/2025;LFT;210100;BRSTNCLF1RL5;05/10/2022;01/03/2029;27;38835;;17834,52582900;17835,54645500;17836,02755000;17789,82057970;17877,62879300;0,0725;0,0733;0,0751;2;7', '01/12/2025;LFT;210100;BRSTNCLF1RM3;05/07/2023;01/09/2029;43;67608;;17812,48271300;17818,55647300;17821,31427300;17767,08299158;17877,62879300;0,0847;0,0888;0,0980;5;12041', '01/12/2025;LFT;210100;BRSTNCLF1RO9;10/01/2024;01/03/2030;56;73748;;17802,23883200;17805,92413700;17817,00575300;17744,41548364;17877,62879300;0,0807;0,0955;0,1004;4;15921']\n"
     ]
    }
   ],
   "source": [
    "for zp in zip_path:\n",
    "    with zipfile.ZipFile(zp, \"r\") as z:\n",
    "        # escolha um arquivo que pareça dados\n",
    "        name = [i.filename for i in z.infolist() if i.filename.lower().endswith((\".csv\", \".txt\"))][0]\n",
    "        print(\"Arquivo:\", name)\n",
    "        with z.open(name) as f:\n",
    "            sample = f.read(4096)  # 4KB\n",
    "        # tente decodificar\n",
    "        for enc in (\"utf-8\", \"latin-1\", \"cp1252\"):\n",
    "            try:\n",
    "                text = sample.decode(enc)\n",
    "                print(\"Encoding provável:\", enc)\n",
    "                print(text.splitlines()[:10])\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623e96b",
   "metadata": {},
   "source": [
    "A partir daí já dá para começar a montar a trusted, pois temos que algumas colunas são fixas e tendo nas datas mais recentes com colunas a mais, irrelevantes para o modelo, mas interessante se atentar na hora de modelar o fluxo para a trusted. Assim como com os dados de SGS podemos já estruturar os dados:\n",
    "\n",
    "**Raw:** Temos muitas colunas que vão apenas ocupar espaço e armazenar informação que nunca será usada para construção de curvas, por mais que seja interessante para analise de liquidez, por exemplo, por enquanto são desnescessárias e não serão carregadas na trusted.\n",
    "| DATA MOV | SIGLA | CODIGO | CODIGO ISIN | EMISSAO | VENCIMENTO | NUM DE OPER | QUANT NEGOCIADA |\n",
    "|:--------:|:-----:|:------:|:-----------:|:-------:|:----------:|:-----------:|:---------------:|\n",
    "\n",
    "| PU MIN | PU MED | PU MAX | PU LASTRO | VALOR PAR | TAXA MIN | TAXA MED | TAXA MAX | OPER COM CORRETAGEM | QUANT NEG COM CORRETAGEM |\n",
    "| :------:|:------:|:------:|:---------:|:---------:|:--------:|:-------:|:--------:|:-------------------:|:------------------------:|\n",
    "\n",
    "**Trusted:**\n",
    "| DATA MOV | SIGLA | CODIGO ISIN | EMISSAO | VENCIMENTO | PU MIN | PU MED | PU MAX | PU LASTRO | VALOR PAR | TAXA MIN | TAXA MED | TAXA MAX |\n",
    "|:--------:|:-----:|:-----------:|:-------:|:----------:|:------:|:------:|:------:|:---------:|:---------:|:--------:|:--------:|:--------:|\n",
    "\n",
    "Já aqui podemos podemos fazer a separação por fato e dimensão e construir os modelos de domínio\n",
    "\n",
    "```python models.py\n",
    "@dataclass(frozen=True)\n",
    "class DemabQuoteDaily:\n",
    "    trade_date: date\n",
    "    codigo_isin: str\n",
    "\n",
    "    pu_min: Optional[float]\n",
    "    pu_med: Optional[float]\n",
    "    pu_max: Optional[float]\n",
    "    pu_lastro: Optional[float] \n",
    "    valor_par : Optional[float]\n",
    "\n",
    "    taxa_min: Optional[float]\n",
    "    taxa_med: Optional[float]\n",
    "    taxa_max: Optional[float]\n",
    "\n",
    "    ref_month: str\n",
    "    raw_zip_file: str\n",
    "    raw_zip_hash: str\n",
    "    inner_file: str\n",
    "    record_hash: str\n",
    "    ingestion_ts_utc: str \n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DemabInstrument:\n",
    "    codigo_isin: str\n",
    "    sigla: str\n",
    "    emissao_date: date\n",
    "    vencimento_date: date\n",
    "    source: str = \"BCB_DEMAB\" # Por padrão por enquanto\n",
    "```\n",
    "onde record_hash será montada por : isin | trade_date | pu_med | taxa_med\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3f878c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 746 entries, 0 to 745\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   trade_date        746 non-null    object \n",
      " 1   isin              746 non-null    str    \n",
      " 2   pu_min            658 non-null    float64\n",
      " 3   pu_med            658 non-null    float64\n",
      " 4   pu_max            658 non-null    float64\n",
      " 5   pu_lastro         746 non-null    float64\n",
      " 6   valor_par         746 non-null    float64\n",
      " 7   taxa_min          425 non-null    float64\n",
      " 8   taxa_med          425 non-null    float64\n",
      " 9   taxa_max          425 non-null    float64\n",
      " 10  raw_zip_file      746 non-null    str    \n",
      " 11  raw_zip_hash      746 non-null    str    \n",
      " 12  inner_file        746 non-null    str    \n",
      " 13  record_hash       746 non-null    str    \n",
      " 14  ingestion_ts_utc  746 non-null    str    \n",
      "dtypes: float64(8), object(1), str(6)\n",
      "memory usage: 228.2+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\02_trusted\\bcb\\demab\\quotes_daily\\2008-08.parquet\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b401b0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([datetime.date(2008, 8, 1), datetime.date(2008, 8, 4),\n",
       "       datetime.date(2008, 8, 5), datetime.date(2008, 8, 6),\n",
       "       datetime.date(2008, 8, 7), datetime.date(2008, 8, 8),\n",
       "       datetime.date(2008, 8, 11), datetime.date(2008, 8, 12),\n",
       "       datetime.date(2008, 8, 13), datetime.date(2008, 8, 14),\n",
       "       datetime.date(2008, 8, 15), datetime.date(2008, 8, 18),\n",
       "       datetime.date(2008, 8, 19), datetime.date(2008, 8, 20),\n",
       "       datetime.date(2008, 8, 21), datetime.date(2008, 8, 22),\n",
       "       datetime.date(2008, 8, 25), datetime.date(2008, 8, 26),\n",
       "       datetime.date(2008, 8, 27), datetime.date(2008, 8, 28),\n",
       "       datetime.date(2008, 8, 29)], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"pu_med\"].isna() & df[\"taxa_med\"].isna()][\"trade_date\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f673027",
   "metadata": {},
   "source": [
    "## *B3 Price Report*\n",
    "Assim como no DEMAB os arquvios B3 são zipados, porém agora cada arquivo representa a extração de um dia. Price Report especificamente contém o relatório completo detalhado por dia do pregão. A decisão de usar esse price report para capturar as negociações de futuro de DI ao invés do simplificado é por conta quantidade de dados históricos disponíveis, já que apenas recentemente começaram a separar o mercado de ações e derivativos em dois reports simplificados distintos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df6c3409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZIP: PR200102_20200102.zip\n",
      "Counter({'.zip': 1})\n",
      "Arquivos: 1\n",
      "- PR200102.zip | 2417.4 KB | 2026-02-15 23:40:14\n",
      "ZIP: PR220517_20220517.zip\n",
      "Counter({'.zip': 1})\n",
      "Arquivos: 1\n",
      "- PR220517.zip | 4915.4 KB | 2026-02-16 00:00:10\n",
      "ZIP: PR260204_20260204.zip\n",
      "Counter({'.zip': 1})\n",
      "Arquivos: 1\n",
      "- PR260204.zip | 8040.9 KB | 2026-02-16 01:10:10\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "zip_path = [Path(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\01_raw\\b3\\PriceReport\\PR200102_20200102.zip\"), \n",
    "            Path(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\01_raw\\b3\\PriceReport\\PR220517_20220517.zip\"),  \n",
    "            Path(r\"C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\01_raw\\b3\\PriceReport\\PR260204_20260204.zip\"),\n",
    "            ]\n",
    "\n",
    "for zp in zip_path:\n",
    "    with zipfile.ZipFile(zp, \"r\") as z:\n",
    "        infos = z.infolist()\n",
    "        print(f\"ZIP: {zp.name}\")\n",
    "        exts = [Path(i.filename).suffix.lower() for i in z.infolist() if not i.is_dir()]\n",
    "        print(Counter(exts))\n",
    "        print(f\"Arquivos: {len(infos)}\")\n",
    "        for i in infos[:50]:\n",
    "            dt = datetime(*i.date_time)\n",
    "            print(f\"- {i.filename} | {i.file_size/1024:.1f} KB | {dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48bb5d7",
   "metadata": {},
   "source": [
    "Muito interessante observar que há dentro de cada arquivo compactado um arquivo compactado ( .zip ) também. Olhando então mais afundo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8aecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ZIP EXTERNO: PR200102_20200102.zip\n",
      "Extensões: Counter({'.zip': 1})\n",
      "Arquivos: 1\n",
      "- PR200102.zip | 2417.4 KB | 2026-02-15 23:40:14\n",
      "\n",
      "  >>> Abrindo ZIP interno: PR200102.zip\n",
      "  Extensões internas: Counter({'.xml': 3})\n",
      "  Arquivos internos: 3\n",
      "   - BVBG.086.01_BV000328202001020328000001830098585.xml | 35566.1 KB | 2020-01-02 18:30:50\n",
      "   - BVBG.086.01_BV000328202001020328000001900552975.xml | 35650.7 KB | 2020-01-02 19:01:34\n",
      "   - BVBG.086.01_BV000328202001020328000001952035761.xml | 35658.0 KB | 2020-01-02 19:52:44\n",
      "\n",
      "ZIP EXTERNO: PR220517_20220517.zip\n",
      "Extensões: Counter({'.zip': 1})\n",
      "Arquivos: 1\n",
      "- PR220517.zip | 4915.4 KB | 2026-02-16 00:00:10\n",
      "\n",
      "  >>> Abrindo ZIP interno: PR220517.zip\n",
      "  Extensões internas: Counter({'.xml': 3})\n",
      "  Arquivos internos: 3\n",
      "   - BVBG.086.01_BV000328202205170328000001809111380.xml | 66746.0 KB | 2022-05-17 18:10:08\n",
      "   - BVBG.086.01_BV000328202205170328000001858502601.xml | 66760.4 KB | 2022-05-17 18:59:38\n",
      "   - BVBG.086.01_BV000328202205170328000001922141813.xml | 66760.4 KB | 2022-05-17 19:23:22\n",
      "\n",
      "ZIP EXTERNO: PR260204_20260204.zip\n",
      "Extensões: Counter({'.zip': 1})\n",
      "Arquivos: 1\n",
      "- PR260204.zip | 8040.9 KB | 2026-02-16 01:10:10\n",
      "\n",
      "  >>> Abrindo ZIP interno: PR260204.zip\n",
      "  Extensões internas: Counter({'.xml': 3})\n",
      "  Arquivos internos: 3\n",
      "   - BVBG.086.01_BV000328202602040328000001849188859.xml | 112042.9 KB | 2026-02-04 18:50:46\n",
      "   - BVBG.086.01_BV000328202602040328000001914114854.xml | 112049.0 KB | 2026-02-04 19:15:36\n",
      "   - BVBG.086.01_BV000328202602040328000001945385434.xml | 112062.6 KB | 2026-02-04 19:47:06\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "for zp in zip_path:\n",
    "    with zipfile.ZipFile(zp, \"r\") as z:\n",
    "        infos = z.infolist()\n",
    "        print(f\"\\nZIP EXTERNO: {zp.name}\")\n",
    "        \n",
    "        exts = [Path(i.filename).suffix.lower() \n",
    "                for i in infos if not i.is_dir()]\n",
    "        print(\"Extensões:\", Counter(exts))\n",
    "        print(f\"Arquivos: {len(infos)}\")\n",
    "\n",
    "        for i in infos:\n",
    "            dt = datetime(*i.date_time)\n",
    "            print(f\"- {i.filename} | {i.file_size/1024:.1f} KB | {dt}\")\n",
    "\n",
    "            # Se for um ZIP interno, abrir na memória\n",
    "            if i.filename.lower().endswith(\".zip\"):\n",
    "                print(f\"\\n  >>> Abrindo ZIP interno: {i.filename}\")\n",
    "\n",
    "                with z.open(i) as inner_file:\n",
    "                    inner_bytes = BytesIO(inner_file.read())\n",
    "\n",
    "                    with zipfile.ZipFile(inner_bytes, \"r\") as inner_zip:\n",
    "                        inner_infos = inner_zip.infolist()\n",
    "                        inner_exts = [\n",
    "                            Path(j.filename).suffix.lower()\n",
    "                            for j in inner_infos if not j.is_dir()\n",
    "                        ]\n",
    "\n",
    "                        print(\"  Extensões internas:\", Counter(inner_exts))\n",
    "                        print(f\"  Arquivos internos: {len(inner_infos)}\")\n",
    "\n",
    "                        for j in inner_infos[:20]:\n",
    "                            dt2 = datetime(*j.date_time)\n",
    "                            print(f\"   - {j.filename} | {j.file_size/1024:.1f} KB | {dt2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd821a",
   "metadata": {},
   "source": [
    "Veja então que consistentemente cada arquivo compactado possui um único arquivo .zip que por sua vez possue 3 arquivos .xml dentro de si. Pela quantidade de arquivos (Estou rodando desde 01/01/2020) é inteligente começar a montar uma estratégia que não carregue (ou carregue o mínimo)desses arquivos na memória e salve apenas o nescessário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "681ebaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<Document xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"urn:bvmf.052.01.xsd bvmf.052.01.xsd\" xmlns=\"urn:bvmf.052.01.xsd\">\n",
      "  <BizFileHdr>\n",
      "    <Xchg>\n",
      "      <BizGrpDesc>\n",
      "        <Fr>\n",
      "          <OrgId>\n",
      "            <Id>\n",
      "              <OrgId>\n",
      "                <Othr>\n",
      "                  <Id>BVMF</Id>\n",
      "                  <Issr>40</Issr>\n",
      "                  <SchmeNm>\n",
      "                    <Prtry>39</Prtry>\n",
      "                  </SchmeNm>\n",
      "                </Othr>\n",
      "              </OrgId>\n",
      "            </Id>\n",
      "          </OrgId>\n",
      "        </Fr>\n",
      "        <To>\n",
      "          <OrgId>\n",
      "            <Id>\n",
      "              <OrgId>\n",
      "                <Othr>\n",
      "                  <Id>PUBLIC</Id>\n",
      "                  <Issr>40</Issr>\n",
      "                  <SchmeNm>\n",
      "                    <Prtry>39</Prtry>\n",
      "                  </SchmeNm>\n",
      "                </Othr>\n",
      "              </OrgId>\n",
      "            </Id>\n",
      "          </OrgId>\n",
      "        </To>\n",
      "        <BizGrpDtls>\n",
      "          <BizGrpIdr>BV000328202001020328000001830040455</BizGrpIdr>\n",
      "          <TtlNbOfMsg>15990</TtlNbOfMsg>\n",
      "          <BizGrpTp>BVBG.086.01</BizGrpTp>\n",
      "          <CreDtAndTm>2020-01-02T18:30:04</CreDtAndTm>\n",
      "        </BizGrpDtls>\n",
      "        <MsgTpDef>\n",
      "          <MsgDefIdr>BVMF.217.01</MsgDefIdr>\n",
      "          <NbOfMsg>15990</NbOfMsg>\n",
      "        </MsgTpDef>\n",
      "      </BizGrpDesc>\n",
      "      <BizGrp>\n",
      "        <AppHdr xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"urn:iso:std:iso:20022:tech:xsd:head.001.001.01\">\n",
      "          <BizMsgIdr>BV000328202001020328000001830040455</BizMsgIdr>\n",
      "          <MsgDefIdr>BVMF.217.01</MsgDefIdr>\n",
      "          <CreDt>2020-01-02T21:30:04Z</CreDt>\n",
      "          <Fr>\n",
      "            <OrgId>\n",
      "              <Id>\n",
      "                <OrgId>\n",
      "                  <Othr>\n",
      "                    <Id>BVMF</Id>\n",
      "                    <SchmeNm>\n",
      "                      <Prtry>39</Prtry>\n",
      "                    </SchmeNm>\n",
      "                    <Issr>40</Issr>\n",
      "                  </Othr>\n",
      "                </OrgId>\n",
      "              </Id>\n",
      "            </OrgId>\n",
      "          </Fr>\n",
      "          <To>\n",
      "            <OrgId>\n",
      "              <Id>\n",
      "                <OrgId>\n",
      "                  <Othr>\n",
      "                    <Id>PUBLIC</Id>\n",
      "                    <SchmeNm>\n",
      "                      <Prtry>39</Prtry>\n",
      "                    </SchmeNm>\n",
      "                    <Issr>40</Issr>\n",
      "                  </Othr>\n",
      "                </OrgId>\n",
      "              </Id>\n",
      "            </OrgId>\n",
      "          </To>\n",
      "        </AppHdr>\n",
      "        <Document xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns=\"urn:bvmf.217.01.xsd\">\n",
      "          <PricRpt>\n",
      "            <TradDt>\n",
      "              <Dt>2020-01-02</Dt>\n",
      "            </TradDt>\n",
      "            <SctyId>\n",
      "              <TckrSymb>IDIJ20P282800</TckrSymb>\n",
      "            </SctyId>\n",
      "            <FinInstrmId>\n",
      "              <OthrId>\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "zip_externo = zip_path[0]\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    xml_name = [n for n in zi.namelist() if n.lower().endswith(\".xml\")][0]\n",
    "    with zi.open(xml_name) as f:\n",
    "        print(f.read(3000).decode(\"utf-8\", errors=\"replace\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c979d868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "# ajuste conforme o padrão real do seu arquivo\n",
    "DI_FUT_RE = re.compile(r\"^DI1\")  # ex.: DI1F26, DI1N26 etc.\n",
    "\n",
    "def strip_ns(tag: str) -> str:\n",
    "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
    "\n",
    "def parse_di_futuro(xml_file):\n",
    "    rows = []\n",
    "    ctx = ET.iterparse(xml_file, events=(\"end\",))\n",
    "\n",
    "    for event, elem in ctx:\n",
    "        if strip_ns(elem.tag) != \"PricRpt\":\n",
    "            continue\n",
    "\n",
    "        # 1) ticker\n",
    "        tck = None\n",
    "        for child in elem.iter():\n",
    "            if strip_ns(child.tag) == \"TckrSymb\":\n",
    "                tck = (child.text or \"\").strip()\n",
    "                break\n",
    "\n",
    "        # filtra cedo\n",
    "        if not tck or not DI_FUT_RE.search(tck):\n",
    "            elem.clear()\n",
    "            continue\n",
    "\n",
    "        # 2) trade date\n",
    "        trade_dt = None\n",
    "        for child in elem.iter():\n",
    "            if strip_ns(child.tag) == \"Dt\":\n",
    "                trade_dt = (child.text or \"\").strip()\n",
    "                break\n",
    "\n",
    "        # 3) aqui você extrai os campos de preço que existirem no seu XML:\n",
    "        # procure por tags típicas como SttlmPric, LastPric, TradPric, etc.\n",
    "        # (depende do schema bvmf.217.01.xsd)\n",
    "        last_price = None\n",
    "        sttl_price = None\n",
    "\n",
    "        for child in elem.iter():\n",
    "            tag = strip_ns(child.tag)\n",
    "            if tag in (\"LastPric\", \"LastPx\"):\n",
    "                last_price = (child.text or \"\").strip()\n",
    "            elif tag in (\"SttlmPric\", \"SttlmPx\", \"SttlmPrice\"):\n",
    "                sttl_price = (child.text or \"\").strip()\n",
    "\n",
    "        rows.append({\n",
    "            \"trade_date\": trade_dt,\n",
    "            \"ticker\": tck,\n",
    "            \"last_price\": last_price,\n",
    "            \"settlement_price\": sttl_price,\n",
    "        })\n",
    "\n",
    "        # libera memória do nó já processado\n",
    "        elem.clear()\n",
    "\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f01479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<Document xmlns:xsd=\"http://www.w3.org/2001/XMLSchema\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"urn:bvmf.052.01.xsd bvmf.052.01.xsd\" xmlns=\"urn:bvmf.052.01.xsd\">\n",
      "  <BizFileHdr>\n",
      "    <Xchg>\n",
      "      <BizGrpDesc>\n",
      "        <Fr>\n",
      "          <OrgId>\n",
      "            <Id>\n",
      "              <OrgId>\n",
      "                <Othr>\n",
      "                  <Id>BVMF</Id>\n",
      "                  <Issr>40</Issr>\n",
      "                  <SchmeNm>\n",
      "                  \n",
      "ocorrências de <?xml: 1\n",
      "últimos 300 bytes:\n",
      "       <AdjstdValCtrct Ccy=\"BRL\">157.5</AdjstdValCtrct>\n",
      "              <MaxTradLmt Ccy=\"BRL\">44.1</MaxTradLmt>\n",
      "              <MinTradLmt Ccy=\"BRL\">39.9</MinTradLmt>\n",
      "            </FinInstrmAttrbts>\n",
      "          </PricRpt>\n",
      "        </Document>\n",
      "      </BizGrp>\n",
      "    </Xchg>\n",
      "  </BizFileHdr>\n",
      "</Document>\n"
     ]
    }
   ],
   "source": [
    "zip_externo = zip_path[0]\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    xml_name = [n for n in zi.namelist() if n.lower().endswith(\".xml\")][0]\n",
    "    with zi.open(xml_name) as f:\n",
    "        head = f.read(500)\n",
    "        print(head.decode(\"utf-8\", errors=\"replace\"))\n",
    "    \n",
    "    with zi.open(xml_name) as f:\n",
    "        data = f.read()  # só para diagnosticar 1x; depois evitamos\n",
    "    print(\"ocorrências de <?xml:\", data.count(b\"<?xml\"))\n",
    "    print(\"últimos 300 bytes:\\n\", data[-300:].decode(\"utf-8\", errors=\"replace\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c149dad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42697a4772703e0d0a202020203c2f586368673e0d0a20203c2f42697a46696c654864723e0d0a3c2f446f63756d656e743e\n",
      "tem NUL (\\x00)? False\n",
      "tamanho do tail após </Document>: 0\n",
      "tail (hex): \n"
     ]
    }
   ],
   "source": [
    "zip_externo = zip_path[0]\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    xml_name = [n for n in zi.namelist() if n.lower().endswith(\".xml\")][0]\n",
    "\n",
    "    raw = zi.read(xml_name)\n",
    "\n",
    "    # últimos 50 bytes em hex\n",
    "    print(raw[-50:].hex())\n",
    "\n",
    "    # verifica se tem NUL\n",
    "    print(\"tem NUL (\\\\x00)?\", b\"\\x00\" in raw[-200:])\n",
    "\n",
    "    # verifica se tem bytes não-whitespace depois do fechamento\n",
    "    end_tag = b\"</Document>\"\n",
    "    pos = raw.rfind(end_tag)\n",
    "    tail = raw[pos + len(end_tag):]\n",
    "    print(\"tamanho do tail após </Document>:\", len(tail))\n",
    "    print(\"tail (hex):\", tail[:80].hex())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4016289a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BVBG.086.01_BV000328202001020328000001830098585.xml count<?xml= 1 tail_len= 0\n",
      "BVBG.086.01_BV000328202001020328000001900552975.xml count<?xml= 1 tail_len= 0\n",
      "BVBG.086.01_BV000328202001020328000001952035761.xml count<?xml= 1 tail_len= 0\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    for xml_name in [n for n in zi.namelist() if n.lower().endswith(\".xml\")]:\n",
    "        raw = zi.read(xml_name)\n",
    "        cnt = raw.count(b\"<?xml\")\n",
    "        end_tag = b\"</Document>\"\n",
    "        pos = raw.rfind(end_tag)\n",
    "        tail = raw[pos + len(end_tag):] if pos != -1 else b\"\"\n",
    "        print(xml_name, \"count<?xml=\", cnt, \"tail_len=\", len(tail))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63f267d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qtde DI1: 37\n",
      "exemplos: ['DI1G20', 'DI1U20', 'DI1N23', 'DI1J20', 'DI1F25', 'DI1X20', 'DI1V24', 'DI1J24', 'DI1F21', 'DI1F31', 'DI1V20', 'DI1N26', 'DI1F29', 'DI1N22', 'DI1N21', 'DI1Q20', 'DI1H20', 'DI1F28', 'DI1J23', 'DI1F24']\n",
      "distintos: 37\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "\n",
    "def strip_ns(tag: str) -> str:\n",
    "    return tag.split(\"}\", 1)[-1] if \"}\" in tag else tag\n",
    "\n",
    "def iter_di1_tickers(xml_file):\n",
    "    \"\"\"\n",
    "    Retorna gerador de tickers que começam com DI1.\n",
    "    Não guarda o XML inteiro; vai limpando memória.\n",
    "    \"\"\"\n",
    "    ctx = ET.iterparse(xml_file, events=(\"end\",))\n",
    "    for event, elem in ctx:\n",
    "        if strip_ns(elem.tag) != \"PricRpt\":\n",
    "            continue\n",
    "\n",
    "        # acha o TckrSymb dentro desse PricRpt\n",
    "        tck = None\n",
    "        for node in elem.iter():\n",
    "            if strip_ns(node.tag) == \"TckrSymb\":\n",
    "                tck = (node.text or \"\").strip()\n",
    "                break\n",
    "\n",
    "        if tck and tck.startswith(\"DI1\"):\n",
    "            yield tck\n",
    "\n",
    "        elem.clear()\n",
    "\n",
    "zip_externo = zip_path[0]\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    xml_name = [n for n in zi.namelist() if n.lower().endswith(\".xml\")][0]\n",
    "\n",
    "    with zi.open(xml_name) as f:\n",
    "        ticks = list(iter_di1_tickers(f))\n",
    "\n",
    "    print(\"qtde DI1:\", len(ticks))\n",
    "    print(\"exemplos:\", ticks[:20])\n",
    "    print(\"distintos:\", len(set(ticks)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51ae648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'by_len': Counter({6: 37}), 'prefix': Counter({'DI1': 37}), 'basic_ok': 37, 'basic_bad': 0, 'examples_ok': ['DI1F20', 'DI1F21', 'DI1F22', 'DI1F23', 'DI1F24', 'DI1F25', 'DI1F26', 'DI1F27', 'DI1F28', 'DI1F29', 'DI1F30', 'DI1F31', 'DI1G20', 'DI1H20', 'DI1J20'], 'examples_bad': []}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def summarize_ticker_patterns(tickers):\n",
    "    by_len = Counter(map(len, tickers))\n",
    "    prefix = Counter(t[:3] for t in tickers)\n",
    "    # captura \"DI1\" + 1 letra + 2 dígitos (padrão muito comum)\n",
    "    re_basic = re.compile(r\"^(DI1)([A-Z])(\\d{2})$\")\n",
    "    ok, bad = [], []\n",
    "    for t in set(tickers):\n",
    "        m = re_basic.match(t)\n",
    "        (ok if m else bad).append(t)\n",
    "\n",
    "    return {\n",
    "        \"by_len\": by_len,\n",
    "        \"prefix\": prefix,\n",
    "        \"basic_ok\": len(ok),\n",
    "        \"basic_bad\": len(bad),\n",
    "        \"examples_ok\": sorted(ok)[:15],\n",
    "        \"examples_bad\": sorted(bad)[:15],\n",
    "    }\n",
    "\n",
    "stats = summarize_ticker_patterns(ticks)\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebfb29cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trade_date': '2020-01-02', 'ticker': 'DI1G20', 'last_price': '4.41', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1U20', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N23', 'last_price': '5.98', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1J20', 'last_price': '4.336', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F25', 'last_price': '6.38', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1X20', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1V24', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1J24', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F21', 'last_price': '4.52', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F31', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1V20', 'last_price': '4.38', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N26', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F29', 'last_price': '6.92', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N22', 'last_price': '5.53', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N21', 'last_price': '4.88', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1Q20', 'last_price': '4.345', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1H20', 'last_price': '4.375', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F28', 'last_price': '6.83', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1J23', 'last_price': '5.89', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F24', 'last_price': '6.16', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F20', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F30', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1J22', 'last_price': '5.41', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N25', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F27', 'last_price': '6.71', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1V23', 'last_price': '6.09', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1K20', 'last_price': '4.31', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F23', 'last_price': '5.77', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N24', 'last_price': '6.3', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1V22', 'last_price': '5.68', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1M20', 'last_price': '4.305', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1N20', 'last_price': '4.315', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1J21', 'last_price': '4.69', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1Z20', 'last_price': None, 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F26', 'last_price': '6.57', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1V21', 'last_price': '5.08', 'settlement_price': None}\n",
      "{'trade_date': '2020-01-02', 'ticker': 'DI1F22', 'last_price': '5.25', 'settlement_price': None}\n"
     ]
    }
   ],
   "source": [
    "def iter_di1_records(xml_file):\n",
    "    ctx = ET.iterparse(xml_file, events=(\"end\",))\n",
    "    for event, elem in ctx:\n",
    "        if strip_ns(elem.tag) != \"PricRpt\":\n",
    "            continue\n",
    "\n",
    "        tck = None\n",
    "        for node in elem.iter():\n",
    "            if strip_ns(node.tag) == \"TckrSymb\":\n",
    "                tck = (node.text or \"\").strip()\n",
    "                break\n",
    "\n",
    "        if not tck or not tck.startswith(\"DI1\"):\n",
    "            elem.clear()\n",
    "            continue\n",
    "\n",
    "        # trade date (primeiro <Dt> que aparece costuma ser a data do pregão)\n",
    "        trade_dt = None\n",
    "        for node in elem.iter():\n",
    "            if strip_ns(node.tag) == \"Dt\":\n",
    "                trade_dt = (node.text or \"\").strip()\n",
    "                break\n",
    "\n",
    "        # IMPORTANTÍSSIMO: essas tags variam por schema. Então pegamos por \"candidatos\".\n",
    "        last_px = None\n",
    "        sttl_px = None\n",
    "        for node in elem.iter():\n",
    "            tag = strip_ns(node.tag)\n",
    "            if tag in (\"LastPric\", \"LastPx\"):\n",
    "                last_px = (node.text or \"\").strip()\n",
    "            elif tag in (\"SttlmPric\", \"SttlmPx\", \"SttlmPrice\"):\n",
    "                sttl_px = (node.text or \"\").strip()\n",
    "\n",
    "        yield {\n",
    "            \"trade_date\": trade_dt,\n",
    "            \"ticker\": tck,\n",
    "            \"last_price\": last_px,\n",
    "            \"settlement_price\": sttl_px,\n",
    "        }\n",
    "\n",
    "        elem.clear()\n",
    "\n",
    "zip_externo = zip_path[0]\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    xml_name = [n for n in zi.namelist() if n.lower().endswith(\".xml\")][0]\n",
    "\n",
    "    with zi.open(xml_name) as f:\n",
    "        ticks_r = list(iter_di1_records(f))\n",
    "print(*ticks_r, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95a7269c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ticker: DI1G20\n",
      "top tags: [('PricRpt', 1), ('TradDt', 1), ('Dt', 1), ('SctyId', 1), ('TckrSymb', 1), ('FinInstrmId', 1), ('OthrId', 1), ('Id', 1), ('Tp', 1), ('Prtry', 1), ('PlcOfListg', 1), ('MktIdrCd', 1), ('TradDtls', 1), ('TradQty', 1), ('FinInstrmAttrbts', 1), ('MktDataStrmId', 1), ('NtlFinVol', 1), ('IntlFinVol', 1), ('OpnIntrst', 1), ('FinInstrmQty', 1), ('BestBidPric', 1), ('BestAskPric', 1), ('FrstPric', 1), ('MinPric', 1), ('MaxPric', 1), ('TradAvrgPric', 1), ('LastPric', 1), ('RglrTxsQty', 1), ('RglrTraddCtrcts', 1), ('NtlRglrVol', 1), ('IntlRglrVol', 1), ('AdjstdQt', 1), ('AdjstdQtTax', 1), ('AdjstdQtStin', 1), ('PrvsAdjstdQt', 1), ('PrvsAdjstdQtTax', 1), ('PrvsAdjstdQtStin', 1), ('OscnPctg', 1), ('VartnPts', 1), ('AdjstdValCtrct', 1)]\n"
     ]
    }
   ],
   "source": [
    "def debug_tags_for_first_di1(xml_file, limit=1):\n",
    "    ctx = ET.iterparse(xml_file, events=(\"end\",))\n",
    "    seen = 0\n",
    "    for event, elem in ctx:\n",
    "        if strip_ns(elem.tag) != \"PricRpt\":\n",
    "            continue\n",
    "\n",
    "        tck = None\n",
    "        for node in elem.iter():\n",
    "            if strip_ns(node.tag) == \"TckrSymb\":\n",
    "                tck = (node.text or \"\").strip()\n",
    "                break\n",
    "\n",
    "        if tck and tck.startswith(\"DI1\"):\n",
    "            tags = Counter(strip_ns(n.tag) for n in elem.iter())\n",
    "            print(\"ticker:\", tck)\n",
    "            print(\"top tags:\", tags.most_common(40))\n",
    "            seen += 1\n",
    "            if seen >= limit:\n",
    "                return\n",
    "        elem.clear()\n",
    "zip_externo = zip_path[0]\n",
    "with zipfile.ZipFile(zip_externo, \"r\") as z:\n",
    "    inner_name = [n for n in z.namelist() if n.lower().endswith(\".zip\")][0]\n",
    "    inner_bytes = BytesIO(z.read(inner_name))\n",
    "\n",
    "with zipfile.ZipFile(inner_bytes, \"r\") as zi:\n",
    "    xml_name = [n for n in zi.namelist() if n.lower().endswith(\".xml\")][0]\n",
    "\n",
    "    with zi.open(xml_name) as f:\n",
    "        debug_tags_for_first_di1(f, limit=1)\n",
    "        f.seek(0)\n",
    "        ticks_r = list(iter_di1_records(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6326696",
   "metadata": {},
   "source": [
    "trade_date, ticker, (e depois maturity)\n",
    "\n",
    "AdjstdValCtrct (settlement/ajuste)\n",
    "\n",
    "BestBidPric, BestAskPric (para construir mid e intervalos)\n",
    "\n",
    "TradQty, OpnIntrst (liquidez)\n",
    "\n",
    "LastPric (útil como diagnóstico)\n",
    "\n",
    "Úteis para diagnósticos e microestrutura\n",
    "\n",
    "TradAvrgPric, MinPric, MaxPric, FrstPric\n",
    "\n",
    "contagens (RglrTxsQty, etc.)\n",
    "\n",
    "OscnPctg, VartnPts (sanity checks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e486bf",
   "metadata": {},
   "source": [
    "## Calendário : *ANBIMA*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fc3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "holidays = pd.read_parquet(r'C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\calendars\\02_trusted\\ref\\anbima_holidays.parquet')\n",
    "bu_index = pd.read_parquet(r'C:\\Users\\Dell\\OneDrive\\Documentos\\GitHub\\ML-ETTJ26\\data\\calendars\\02_trusted\\ref\\calendar_bd_index.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76924c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 1263 entries, 0 to 1262\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   cal_id            1263 non-null   str   \n",
      " 1   date              1263 non-null   object\n",
      " 2   holiday_name      1263 non-null   str   \n",
      " 3   weekday           1263 non-null   int32 \n",
      " 4   ingestion_ts_utc  1263 non-null   str   \n",
      " 5   source_file_hash  1263 non-null   str   \n",
      " 6   pipeline_run_id   1263 non-null   str   \n",
      "dtypes: int32(1), object(1), str(5)\n",
      "memory usage: 214.4+ KB\n",
      "<class 'pandas.DataFrame'>\n",
      "RangeIndex: 36159 entries, 0 to 36158\n",
      "Data columns (total 9 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   cal_id            36159 non-null  str   \n",
      " 1   date              36159 non-null  object\n",
      " 2   weekday           36159 non-null  int32 \n",
      " 3   is_business_day   36159 non-null  bool  \n",
      " 4   bd_index          36159 non-null  int64 \n",
      " 5   holiday_name      1263 non-null   str   \n",
      " 6   ingestion_ts_utc  36159 non-null  str   \n",
      " 7   source_file_hash  36159 non-null  str   \n",
      " 8   pipeline_run_id   36159 non-null  str   \n",
      "dtypes: bool(1), int32(1), int64(1), object(1), str(5)\n",
      "memory usage: 5.7+ MB\n",
      "None\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(holidays.info(), bu_index.info(), sep=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22e3baed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cal_id</th>\n",
       "      <th>date</th>\n",
       "      <th>weekday</th>\n",
       "      <th>is_business_day</th>\n",
       "      <th>bd_index</th>\n",
       "      <th>holiday_name</th>\n",
       "      <th>ingestion_ts_utc</th>\n",
       "      <th>source_file_hash</th>\n",
       "      <th>pipeline_run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>Confraternização Universal</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-03</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-04</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-05</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-06</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-07</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-08</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-09</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BR_ANBIMA</td>\n",
       "      <td>2001-01-10</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2026-02-21T13:54:11+00:00</td>\n",
       "      <td>bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...</td>\n",
       "      <td>local</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cal_id        date  weekday  is_business_day  bd_index  \\\n",
       "0  BR_ANBIMA  2001-01-01        0            False         0   \n",
       "1  BR_ANBIMA  2001-01-02        1             True         1   \n",
       "2  BR_ANBIMA  2001-01-03        2             True         2   \n",
       "3  BR_ANBIMA  2001-01-04        3             True         3   \n",
       "4  BR_ANBIMA  2001-01-05        4             True         4   \n",
       "5  BR_ANBIMA  2001-01-06        5            False         4   \n",
       "6  BR_ANBIMA  2001-01-07        6            False         4   \n",
       "7  BR_ANBIMA  2001-01-08        0             True         5   \n",
       "8  BR_ANBIMA  2001-01-09        1             True         6   \n",
       "9  BR_ANBIMA  2001-01-10        2             True         7   \n",
       "\n",
       "                 holiday_name           ingestion_ts_utc  \\\n",
       "0  Confraternização Universal  2026-02-21T13:54:11+00:00   \n",
       "1                         NaN  2026-02-21T13:54:11+00:00   \n",
       "2                         NaN  2026-02-21T13:54:11+00:00   \n",
       "3                         NaN  2026-02-21T13:54:11+00:00   \n",
       "4                         NaN  2026-02-21T13:54:11+00:00   \n",
       "5                         NaN  2026-02-21T13:54:11+00:00   \n",
       "6                         NaN  2026-02-21T13:54:11+00:00   \n",
       "7                         NaN  2026-02-21T13:54:11+00:00   \n",
       "8                         NaN  2026-02-21T13:54:11+00:00   \n",
       "9                         NaN  2026-02-21T13:54:11+00:00   \n",
       "\n",
       "                                    source_file_hash pipeline_run_id  \n",
       "0  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "1  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "2  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "3  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "4  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "5  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "6  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "7  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "8  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  \n",
       "9  bff506cbcab013530f5b522ad8b0b2cec0c967dce9b245...           local  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bu_index.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
